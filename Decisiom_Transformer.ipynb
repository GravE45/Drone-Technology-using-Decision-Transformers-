{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import MultiDiscrete, Box\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model, GPT2Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reward_list = []\n",
    "GLOBAL_UAV1_PATH = []\n",
    "GLOBAL_UAV2_PATH = []\n",
    "pre_total_reward = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m GLOBAL_UAV2_PATH \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m pre_total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01muavEnv\u001b[39;00m(Env):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_lau_location  \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m950\u001b[39m, \u001b[38;5;241m50\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Env' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "reward_list= []\n",
    "GLOBAL_UAV1_PATH = []\n",
    "GLOBAL_UAV2_PATH = []\n",
    "pre_total_reward = 1000\n",
    "class uavEnv(Env):\n",
    "    def __init__(self):\n",
    "        self.X_lau_location  = [950, 50]\n",
    "        self.Y_lau_location  = [950, 50]\n",
    "        self.UAV1_PATH = []\n",
    "        self.UAV2_PATH = []\n",
    "        self.UAV3_PATH = []\n",
    "        self.UAV4_PATH = []\n",
    "        self.start_time = time.time()\n",
    "        self.g31_capacity = {}\n",
    "        self.completed = 0\n",
    "        self.account = 0 \n",
    "        self.E = 0\n",
    "        self.action_space = MultiDiscrete([5,5]) \n",
    "        self.observation_space = Box(low=np.array([-100]*4),high=np.array([1000]*4)) \n",
    "        self.state = np.array([10]*4)\n",
    "        self.state[0] = self.X_lau_location[0]  \n",
    "        self.state[1] = self.Y_lau_location[0]\n",
    "        self.state[2] = self.X_lau_location[1]\n",
    "        self.state[3] = self.Y_lau_location[1]     \n",
    "        # number of users\n",
    "        self.User_Location_X = [399, 91, 750, 956, 393, 790, 728, 211, 565, 971, 576, 266, 571, 401, 875, 837, 575, 585, 625, 788, 810, 792, 499, 140, 816, 391, 29, 36, 171, 530, 58, 546, 205, 89, 964, 117, 818, 64, 680, 981]\n",
    "        self.User_Location_Y = [29, 590, 9, 448, 480, 633, 862, 588, 860, 103, 680, 86, 911, 948, 198, 933, 824, 270, 903, 825, 380, 641, 444, 725, 630, 150, 852, 382, 362, 463, 722, 621, 971, 420, 863, 853, 807, 317, 699, 991]\n",
    "\n",
    "        Number_User = 40\n",
    "        self.Max_flow_total = 0\n",
    "        self.total_reward = 0\n",
    "\n",
    "        \n",
    "    def step(self, Action):\n",
    "\n",
    "        M_LAU_length = 50\n",
    "        Number_LAU = 2\n",
    "        pently_lau = 0\n",
    "        \n",
    "    \n",
    "\n",
    "        for i in range(Number_LAU):\n",
    "            if Action[i] == 0:  #hover\n",
    "                self.X_lau_location[i] = self.X_lau_location[i]\n",
    "                self.Y_lau_location[i] = self.Y_lau_location[i]\n",
    "\n",
    "            elif Action[i] == 1: # back\n",
    "                self.Y_lau_location[i] = self.Y_lau_location[i] - M_LAU_length\n",
    "                if self.Y_lau_location[i]<0: # Out of range\n",
    "                    pently_lau = 1\n",
    "                    self.Y_lau_location[i] = self.Y_lau_location[i] + M_LAU_length\n",
    "\n",
    "            elif Action[i] == 2: # forward\n",
    "                self.Y_lau_location[i] = self.Y_lau_location[i] + M_LAU_length\n",
    "                if self.Y_lau_location[i]>1000:\n",
    "                    pently_lau = 1\n",
    "                    self.Y_lau_location[i] = self.Y_lau_location[i] - M_LAU_length\n",
    "\n",
    "            elif Action[i] == 3: # left\n",
    "                self.X_lau_location[i] = self.X_lau_location[i] - M_LAU_length\n",
    "                if self.X_lau_location[i]<0:\n",
    "                    pently_lau = 1\n",
    "                    self.X_lau_location[i] = self.X_lau_location[i] + M_LAU_length\n",
    "\n",
    "            elif Action[i] == 4: # right\n",
    "                self.X_lau_location[i] = self.X_lau_location[i]+ M_LAU_length\n",
    "                if self.X_lau_location[i]>1000:\n",
    "                    pently_lau = 1\n",
    "                    self.X_lau_location[i] = self.X_lau_location[i] - M_LAU_length\n",
    "\n",
    "        Number_User =  len(self.User_Location_X)\n",
    "        R_User_Location_X = []\n",
    "        R_User_Location_Y = []\n",
    "        Current_reward = 0\n",
    "        for j in range(Number_User):\n",
    "            for i in range(Number_LAU):\n",
    "                D_GL = math.sqrt((self.User_Location_X[j]-self.X_lau_location[i])**2+(self.User_Location_Y[j]-self.Y_lau_location[i])**2)/1000\n",
    "                if D_GL<0.1: # coverage of UAV :100m\n",
    "                    self.completed = self.completed + 1\n",
    "                    Current_reward = Current_reward + 10\n",
    "                    R_User_Location_X.append(self.User_Location_X[j])\n",
    "                    R_User_Location_Y.append(self.User_Location_Y[j])\n",
    "\n",
    "            \n",
    "          \n",
    "        self.User_Location_X = [x for x in self.User_Location_X if x not in R_User_Location_X]     \n",
    "        self.User_Location_Y = [x for x in self.User_Location_Y if x not in R_User_Location_Y]   \n",
    "                    \n",
    "                    \n",
    "        \n",
    "        self.account = self.account + 1\n",
    "        self.state[0] = self.X_lau_location[0]  \n",
    "        self.state[1] = self.Y_lau_location[0]\n",
    "        self.state[2] = self.X_lau_location[1]\n",
    "        self.state[3] = self.Y_lau_location[1]     \n",
    "        U1 =[self.X_lau_location[0],self.Y_lau_location[0]]\n",
    "        U2 =[self.X_lau_location[1],self.Y_lau_location[1]]\n",
    "        self.UAV1_PATH.append(U1)\n",
    "        self.UAV2_PATH.append(U2)\n",
    "\n",
    "        \n",
    "        \n",
    "        reward =  Current_reward-(pently_lau)*100\n",
    "        ##print(reward,self.account,self.completed)\n",
    "        self.total_reward = self.total_reward + reward\n",
    "        if self.account >= 200: # maximum step of UAVs\n",
    "            done = True\n",
    "            print(self.completed)\n",
    "            reward_list.append(self.total_reward)\n",
    "            info = {}\n",
    "            return self.state, reward, done, info\n",
    "        \n",
    "        else:\n",
    "            done = False\n",
    "            \n",
    "        global pre_total_reward\n",
    "        global GLOBAL_UAV1_PATH\n",
    "        global GLOBAL_UAV2_PATH\n",
    "        if self.completed >= 40:\n",
    "            done = True\n",
    "            reward = reward+ 20*(100-self.account)\n",
    "            self.total_reward = self.total_reward + reward\n",
    "            if self.account < pre_total_reward :\n",
    "                pre_total_reward = self.account\n",
    "                # trajectory of UAVs\n",
    "                GLOBAL_UAV1_PATH = self.UAV1_PATH\n",
    "                GLOBAL_UAV2_PATH = self.UAV2_PATH\n",
    "            reward_list.append(self.total_reward)\n",
    "            info = {}\n",
    "            print(self.account,self.total_reward)\n",
    "            return self.state, reward, done, info\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "\n",
    "        info = {}\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        pass \n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.account = 0\n",
    "        self.completed = 0\n",
    "        self.total_reward = 0\n",
    "        self.Max_flow_total = 0\n",
    "        self.average_delay = 0\n",
    "        self.X_lau_location = []\n",
    "        self.Y_lau_location = []\n",
    "        # reset the initial locations of UAVs\n",
    "        for _ in range(2):\n",
    "            self.X_lau_location.append(random.randint(0, 1000))\n",
    "            self.Y_lau_location.append(random.randint(0, 1000))\n",
    "            \n",
    "        self.g31_capacity = {}\n",
    "        self.UAV1_PATH = []\n",
    "        self.UAV2_PATH = []\n",
    "        self.UAV3_PATH = []\n",
    "        self.UAV4_PATH = []\n",
    "        Number_User = 40\n",
    "        self.User_Location_X = [399, 91, 750, 956, 393, 790, 728, 211, 565, 971, 576, 266, 571, 401, 875, 837, 575, 585, 625, 788, 810, 792, 499, 140, 816, 391, 29, 36, 171, 530, 58, 546, 205, 89, 964, 117, 818, 64, 680, 981]\n",
    "        self.User_Location_Y = [29, 590, 9, 448, 480, 633, 862, 588, 860, 103, 680, 86, 911, 948, 198, 933, 824, 270, 903, 825, 380, 641, 444, 725, 630, 150, 852, 382, 362, 463, 722, 621, 971, 420, 863, 853, 807, 317, 699, 991]\n",
    "        self.E = 0\n",
    "        self.E_CC = 0\n",
    "        self.state[0] = self.X_lau_location[0]  \n",
    "        self.state[1] = self.Y_lau_location[0]\n",
    "        self.state[2] = self.X_lau_location[1]\n",
    "        self.state[3] = self.Y_lau_location[1]     \n",
    "        return self.state\n",
    "    \n",
    "env = uavEnv()\n",
    "\n",
    "def collect_trajectories(env, num_trajectories):\n",
    "    trajectories = []\n",
    "    for _ in range(num_trajectories):\n",
    "        state = env.reset()\n",
    "        trajectory = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "        trajectories.append(trajectory)\n",
    "    return trajectories\n",
    "\n",
    "trajectories = collect_trajectories(env, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Decision Transformer Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "class DecisionTransformer(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_length=50):\n",
    "        super(DecisionTransformer, self).__init__()\n",
    "        config = GPT2Config(vocab_size=1, n_positions=max_length, n_embd=128, n_layer=4, n_head=8)\n",
    "        self.transformer = GPT2Model(config)\n",
    "        self.state_emb = nn.Linear(state_dim, config.n_embd)\n",
    "        self.action_emb = nn.Linear(action_dim, config.n_embd)\n",
    "        self.reward_emb = nn.Linear(1, config.n_embd)\n",
    "        self.predict_action = nn.Linear(config.n_embd, action_dim)\n",
    "        \n",
    "    def forward(self, states, actions, rewards):\n",
    "        state_embeddings = self.state_emb(states)\n",
    "        action_embeddings = self.action_emb(actions)\n",
    "        reward_embeddings = self.reward_emb(rewards.unsqueeze(-1))\n",
    "        \n",
    "        inputs = state_embeddings + action_embeddings + reward_embeddings\n",
    "        outputs = self.transformer(inputs)\n",
    "        predicted_actions = self.predict_action(outputs)\n",
    "        return predicted_actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training The Decision Transformer\n",
    "def preprocess_trajectories(trajectories):\n",
    "    states, actions, rewards = [], [], []\n",
    "    for traj in trajectories:\n",
    "        states.append(np.array([step[0] for step in traj]))\n",
    "        actions.append(np.array([step[1] for step in traj]))\n",
    "        rewards.append(np.array([step[2] for step in traj]))\n",
    "    return np.array(states), np.array(actions), np.array(rewards)\n",
    "\n",
    "states, actions, rewards = preprocess_trajectories(trajectories)\n",
    "\n",
    "model = DecisionTransformer(state_dim=4, action_dim=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(1000):  # Number of epochs\n",
    "    for i in range(len(states)):\n",
    "        state_batch = torch.tensor(states[i], dtype=torch.float32)\n",
    "        action_batch = torch.tensor(actions[i], dtype=torch.float32)\n",
    "        reward_batch = torch.tensor(rewards[i], dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predicted_actions = model(state_batch, action_batch, reward_batch)\n",
    "        loss = loss_fn(predicted_actions, action_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch {epoch}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate The Model\n",
    "def evaluate_model(env, model):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        action = model(state_tensor, torch.zeros_like(state_tensor), torch.zeros(1, 1)).argmax(dim=-1).item()\n",
    "        next_state, reward, done, _ = env.step([action])\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "reward = evaluate_model(env, model)\n",
    "print(f'Total Reward: {reward}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
