{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "returns shape: (2, 5)\n",
      "states shape: (2, 5, 2)\n",
      "actions shape: (2, 5, 2)\n",
      "timesteps shape: (2, 5)\n",
      "returns_to_go shape: (2, 5)\n",
      "returns_tensor shape: (2, 5)\n",
      "states_tensor shape: (2, 5, 2)\n",
      "actions_tensor shape: (2, 5, 2)\n",
      "timesteps_tensor shape: (2, 5)\n",
      "returns_to_go_tensor shape: (2, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Example trajectories\n",
    "# Each inner list is a sequence of (state, action, reward, next_state, done) tuples\n",
    "trajectories = [\n",
    "    [\n",
    "        ([1.0, 2.0], [0.1, 0.2], 1.0, [1.5, 2.5], False),\n",
    "        ([1.5, 2.5], [0.1, 0.2], 0.5, [2.0, 3.0], False),\n",
    "        ([2.0, 3.0], [0.1, 0.2], -1.0, [2.5, 3.5], True)\n",
    "    ],\n",
    "    [\n",
    "        ([0.5, 1.5], [0.2, 0.3], 1.5, [1.0, 2.0], False),\n",
    "        ([1.0, 2.0], [0.2, 0.3], -0.5, [1.5, 2.5], True)\n",
    "    ]\n",
    "]\n",
    "\n",
    "def preprocess_trajectories(trajectories, state_dim, action_dim, max_len):\n",
    "    returns, states, actions, timesteps = [], [], [], []\n",
    "    for traj in trajectories:\n",
    "        traj_returns = []\n",
    "        traj_states = []\n",
    "        traj_actions = []\n",
    "        traj_timesteps = []\n",
    "        for t, step in enumerate(traj):\n",
    "            state, action, reward, next_state, done = step\n",
    "            traj_returns.append(reward)\n",
    "            traj_states.append(state)\n",
    "            traj_actions.append(action)\n",
    "            traj_timesteps.append(t)\n",
    "        \n",
    "        # Pad sequences to the maximum length\n",
    "        while len(traj_returns) < max_len:\n",
    "            traj_returns.append(0.0)  # Assuming padding with 0 for rewards\n",
    "            traj_states.append([0.0] * state_dim)  # Assuming padding with 0 for states\n",
    "            traj_actions.append([0.0] * action_dim)  # Assuming padding with 0 for actions\n",
    "            traj_timesteps.append(0)  # Assuming padding with 0 for timesteps\n",
    "        \n",
    "        returns.append(traj_returns)\n",
    "        states.append(traj_states)\n",
    "        actions.append(traj_actions)\n",
    "        timesteps.append(traj_timesteps)\n",
    "    \n",
    "    returns_array = np.array(returns)\n",
    "    states_array = np.array(states)\n",
    "    actions_array = np.array(actions)\n",
    "    timesteps_array = np.array(timesteps)\n",
    "    \n",
    "    # Calculate returns-to-go\n",
    "    returns_to_go = []\n",
    "    for traj in returns_array:\n",
    "        traj_returns_to_go = np.flip(np.cumsum(np.flip(traj, axis=0)), axis=0)\n",
    "        returns_to_go.append(traj_returns_to_go)\n",
    "    \n",
    "    returns_to_go_array = np.array(returns_to_go)\n",
    "    \n",
    "    print(f'returns shape: {returns_array.shape}')\n",
    "    print(f'states shape: {states_array.shape}')\n",
    "    print(f'actions shape: {actions_array.shape}')\n",
    "    print(f'timesteps shape: {timesteps_array.shape}')\n",
    "    print(f'returns_to_go shape: {returns_to_go_array.shape}')\n",
    "    \n",
    "    return returns_array, states_array, actions_array, timesteps_array, returns_to_go_array\n",
    "\n",
    "# Define the dimensions and maximum length\n",
    "state_dim = 2\n",
    "action_dim = 2\n",
    "max_len = 5  # Assuming a maximum length of 5 for simplicity\n",
    "\n",
    "# Preprocess the data\n",
    "returns, states, actions, timesteps, returns_to_go = preprocess_trajectories(trajectories, state_dim, action_dim, max_len)\n",
    "\n",
    "# Convert to TensorFlow tensors\n",
    "returns_tensor = tf.convert_to_tensor(returns, dtype=tf.float32)\n",
    "states_tensor = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "actions_tensor = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "timesteps_tensor = tf.convert_to_tensor(timesteps, dtype=tf.float32)\n",
    "returns_to_go_tensor = tf.convert_to_tensor(returns_to_go, dtype=tf.float32)\n",
    "\n",
    "print(f'returns_tensor shape: {returns_tensor.shape}')\n",
    "print(f'states_tensor shape: {states_tensor.shape}')\n",
    "print(f'actions_tensor shape: {actions_tensor.shape}')\n",
    "print(f'timesteps_tensor shape: {timesteps_tensor.shape}')\n",
    "print(f'returns_to_go_tensor shape: {returns_to_go_tensor.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.activations'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdecision_transformer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrajectoryModel\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdecision_transformer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrajectory_gpt2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Model\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDecisionTransformer\u001b[39;00m(TrajectoryModel):\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    This model uses GPT to model (Return_1, state_1, action_1, Return_2, state_2, ...)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\steve\\OneDrive\\Desktop\\AI _PROJECT_ UAV\\decision_transformer\\models\\trajectory_gpt2.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEntropyLoss, MSELoss\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ACT2FN\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     ModelOutput,\n\u001b[0;32m     29\u001b[0m     add_code_sample_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     36\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers.activations'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "\n",
    "from decision_transformer.models.model import TrajectoryModel\n",
    "from decision_transformer.models.trajectory_gpt2 import GPT2Model\n",
    "\n",
    "\n",
    "class DecisionTransformer(TrajectoryModel):\n",
    "\n",
    "    \"\"\"\n",
    "    This model uses GPT to model (Return_1, state_1, action_1, Return_2, state_2, ...)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            state_dim,\n",
    "            act_dim,\n",
    "            hidden_size,\n",
    "            max_length=None,\n",
    "            max_ep_len=4096,\n",
    "            action_tanh=True,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(state_dim, act_dim, max_length=max_length)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        config = transformers.GPT2Config(\n",
    "            vocab_size=1,  # doesn't matter -- we don't use the vocab\n",
    "            n_embd=hidden_size,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # note: the only difference between this GPT2Model and the default Huggingface version\n",
    "        # is that the positional embeddings are removed (since we'll add those ourselves)\n",
    "        self.transformer = GPT2Model(config)\n",
    "\n",
    "        self.embed_timestep = nn.Embedding(max_ep_len, hidden_size)\n",
    "        self.embed_return = torch.nn.Linear(1, hidden_size)\n",
    "        self.embed_state = torch.nn.Linear(self.state_dim, hidden_size)\n",
    "        self.embed_action = torch.nn.Linear(self.act_dim, hidden_size)\n",
    "\n",
    "        self.embed_ln = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # note: we don't predict states or returns for the paper\n",
    "        self.predict_state = torch.nn.Linear(hidden_size, self.state_dim)\n",
    "        self.predict_action = nn.Sequential(\n",
    "            *([nn.Linear(hidden_size, self.act_dim)] + ([nn.Tanh()] if action_tanh else []))\n",
    "        )\n",
    "        self.predict_return = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, states, actions, rewards, returns_to_go, timesteps, attention_mask=None):\n",
    "\n",
    "        batch_size, seq_length = states.shape[0], states.shape[1]\n",
    "\n",
    "        if attention_mask is None:\n",
    "            # attention mask for GPT: 1 if can be attended to, 0 if not\n",
    "            attention_mask = torch.ones((batch_size, seq_length), dtype=torch.long)\n",
    "\n",
    "        # embed each modality with a different head\n",
    "        state_embeddings = self.embed_state(states)\n",
    "        action_embeddings = self.embed_action(actions)\n",
    "        returns_embeddings = self.embed_return(returns_to_go)\n",
    "        time_embeddings = self.embed_timestep(timesteps)\n",
    "\n",
    "        # time embeddings are treated similar to positional embeddings\n",
    "        state_embeddings = state_embeddings + time_embeddings\n",
    "        action_embeddings = action_embeddings + time_embeddings\n",
    "        returns_embeddings = returns_embeddings + time_embeddings\n",
    "\n",
    "        # this makes the sequence look like (R_1, s_1, a_1, R_2, s_2, a_2, ...)\n",
    "        # which works nice in an autoregressive sense since states predict actions\n",
    "        stacked_inputs = torch.stack(\n",
    "            (returns_embeddings, state_embeddings, action_embeddings), dim=1\n",
    "        ).permute(0, 2, 1, 3).reshape(batch_size, 3*seq_length, self.hidden_size)\n",
    "        stacked_inputs = self.embed_ln(stacked_inputs)\n",
    "\n",
    "        # to make the attention mask fit the stacked inputs, have to stack it as well\n",
    "        stacked_attention_mask = torch.stack(\n",
    "            (attention_mask, attention_mask, attention_mask), dim=1\n",
    "        ).permute(0, 2, 1).reshape(batch_size, 3*seq_length)\n",
    "\n",
    "        # we feed in the input embeddings (not word indices as in NLP) to the model\n",
    "        transformer_outputs = self.transformer(\n",
    "            inputs_embeds=stacked_inputs,\n",
    "            attention_mask=stacked_attention_mask,\n",
    "        )\n",
    "        x = transformer_outputs['last_hidden_state']\n",
    "\n",
    "        # reshape x so that the second dimension corresponds to the original\n",
    "        # returns (0), states (1), or actions (2); i.e. x[:,1,t] is the token for s_t\n",
    "        x = x.reshape(batch_size, seq_length, 3, self.hidden_size).permute(0, 2, 1, 3)\n",
    "\n",
    "        # get predictions\n",
    "        return_preds = self.predict_return(x[:,2])  # predict next return given state and action\n",
    "        state_preds = self.predict_state(x[:,2])    # predict next state given state and action\n",
    "        action_preds = self.predict_action(x[:,1])  # predict next action given state\n",
    "\n",
    "        return state_preds, action_preds, return_preds\n",
    "\n",
    "    def get_action(self, states, actions, rewards, returns_to_go, timesteps, **kwargs):\n",
    "        # we don't care about the past rewards in this model\n",
    "\n",
    "        states = states.reshape(1, -1, self.state_dim)\n",
    "        actions = actions.reshape(1, -1, self.act_dim)\n",
    "        returns_to_go = returns_to_go.reshape(1, -1, 1)\n",
    "        timesteps = timesteps.reshape(1, -1)\n",
    "\n",
    "        if self.max_length is not None:\n",
    "            states = states[:,-self.max_length:]\n",
    "            actions = actions[:,-self.max_length:]\n",
    "            returns_to_go = returns_to_go[:,-self.max_length:]\n",
    "            timesteps = timesteps[:,-self.max_length:]\n",
    "\n",
    "            # pad all tokens to sequence length\n",
    "            attention_mask = torch.cat([torch.zeros(self.max_length-states.shape[1]), torch.ones(states.shape[1])])\n",
    "            attention_mask = attention_mask.to(dtype=torch.long, device=states.device).reshape(1, -1)\n",
    "            states = torch.cat(\n",
    "                [torch.zeros((states.shape[0], self.max_length-states.shape[1], self.state_dim), device=states.device), states],\n",
    "                dim=1).to(dtype=torch.float32)\n",
    "            actions = torch.cat(\n",
    "                [torch.zeros((actions.shape[0], self.max_length - actions.shape[1], self.act_dim),\n",
    "                             device=actions.device), actions],\n",
    "                dim=1).to(dtype=torch.float32)\n",
    "            returns_to_go = torch.cat(\n",
    "                [torch.zeros((returns_to_go.shape[0], self.max_length-returns_to_go.shape[1], 1), device=returns_to_go.device), returns_to_go],\n",
    "                dim=1).to(dtype=torch.float32)\n",
    "            timesteps = torch.cat(\n",
    "                [torch.zeros((timesteps.shape[0], self.max_length-timesteps.shape[1]), device=timesteps.device), timesteps],\n",
    "                dim=1\n",
    "            ).to(dtype=torch.long)\n",
    "        else:\n",
    "            attention_mask = None\n",
    "\n",
    "        _, action_preds, return_preds = self.forward(\n",
    "            states, actions, None, returns_to_go, timesteps, attention_mask=attention_mask, **kwargs)\n",
    "\n",
    "        return action_preds[0,-1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
